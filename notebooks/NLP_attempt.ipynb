{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'comment_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3359'>3360</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'comment_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39m# Split into Train and Test data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39m#(X, Y, test_size=0.3, random_state=42)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000003?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mpop(\u001b[39m'\u001b[39;49m\u001b[39mcomment_text\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000003?line=3'>4</a>\u001b[0m df\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000003?line=4'>5</a>\u001b[0m y \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msum(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto_frame(name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoxic\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py:5226\u001b[0m, in \u001b[0;36mDataFrame.pop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5184'>5185</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpop\u001b[39m(\u001b[39mself\u001b[39m, item: Hashable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5185'>5186</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5186'>5187</a>\u001b[0m \u001b[39m    Return item and drop from frame. Raise KeyError if not found.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5187'>5188</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5223'>5224</a>\u001b[0m \u001b[39m    3  monkey        NaN\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5224'>5225</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=5225'>5226</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpop(item\u001b[39m=\u001b[39;49mitem)\n",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/generic.py:870\u001b[0m, in \u001b[0;36mNDFrame.pop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/generic.py?line=868'>869</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpop\u001b[39m(\u001b[39mself\u001b[39m, item: Hashable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m Any:\n\u001b[0;32m--> <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/generic.py?line=869'>870</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[item]\n\u001b[1;32m    <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/generic.py?line=870'>871</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m[item]\n\u001b[1;32m    <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/generic.py?line=872'>873</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3455'>3456</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3456'>3457</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3457'>3458</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3458'>3459</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/frame.py?line=3459'>3460</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3362'>3363</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3364'>3365</a>\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[1;32m   <a href='file:///Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py?line=3365'>3366</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'comment_text'"
     ]
    }
   ],
   "source": [
    "# Split into Train and Test data\n",
    "#(X, Y, test_size=0.3, random_state=42)\n",
    "X = df.pop('comment_text')\n",
    "df.pop('id')\n",
    "y = df.sum(axis = 1).to_frame(name = 'toxic')\n",
    "y.loc[y['toxic'] >0,'toxic'] = 1\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,test_size=0.3)\n",
    "Encoder = LabelEncoder()\n",
    "# Make Labels\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vectorized: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<111699x153097 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4855587 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(\"X_train_vectorized: \")\n",
    "X_train_vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (111699,)\n",
      "Vocabulary length = 153097\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"Vocabulary length = {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[:20]\n",
    "print(X_train_vectorized.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.837\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['weve' 'sikhs' 'nambla' 'catches' 'extensive' 'brighton' 'sniping'\n",
      " 'letting' 'ground' 'semi']\n",
      "\n",
      "Largest Coefs: \n",
      "['fuck' 'fucking' 'bitch' 'asshole' 'idiot' 'faggot' 'ass' 'shit' 'suck'\n",
      " 'sucks']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model (from lowest to highest values)\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('08', 10),\n",
       " ('084080', 11),\n",
       " ('09', 12),\n",
       " ('0px', 13),\n",
       " ('10', 14),\n",
       " ('100', 15),\n",
       " ('1000', 16),\n",
       " ('101', 17),\n",
       " ('102', 18),\n",
       " ('103', 19),\n",
       " ('104', 20),\n",
       " ('105', 21),\n",
       " ('106', 22),\n",
       " ('107', 23),\n",
       " ('108', 24),\n",
       " ('109', 25),\n",
       " ('10th', 26),\n",
       " ('11', 27),\n",
       " ('110', 28),\n",
       " ('111', 29)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 15\n",
    "# This means a word should have been used in at least 15 SMS \n",
    "vect = TfidfVectorizer(min_df=15).fit(X_train)\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# let's look of some of the words gathered with this method\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14307"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear in more than 15 text messages\n",
    "len(sorted(vect.vocabulary_.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['templatename' 'signified' 'banished' 'conformance' 'seashell' '084080'\n",
      " 'demean' 'fair_use' 'annually' 'striving']\n",
      "\n",
      "Largest tfidf: \n",
      "['stop' 'mean' 'am' 'faggot' 'wright' 'hey' 'faggots' 'fags' 'singer' 'hi']\n"
     ]
    }
   ],
   "source": [
    "# save all feature names == words in an array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "#sort for the column names according to highest tfidf value in the column\n",
    "sorted_tfidf_index = X_train_vectorized.toarray().max(0).argsort()\n",
    "\n",
    "# print words with highest and lowest tfidf values\n",
    "print(\"Smallest tfidf:\\n{}\\n\".format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print(\"Largest tfidf: \\n{}\".format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.803\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1500)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['thank' 'talk' 'thanks' 'please' 'if' 'redirect' 'for' 'help' 'continue'\n",
      " 'welcome']\n",
      "\n",
      "Largest Coefs: \n",
      "['fuck' 'fucking' 'shit' 'idiot' 'stupid' 'ass' 'bullshit' 'asshole'\n",
      " 'crap' 'suck']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# define ContVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_stem_vectorized = stem_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.843\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_stemm = LogisticRegression(max_iter=1500)\n",
    "model_stemm.fit(X_train_stem_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model_stemm.predict(stem_vectorizer.transform(X_test))\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['nambla' 'stress' 'mistress' 'brighton' 'mutual' 'papphas' 'belittl'\n",
      " 'gregalton' 'recommend' 'weve']\n",
      "\n",
      "Largest Coefs: \n",
      "['fuck' 'faggot' 'asshol' 'bitch' 'suck' 'idiot' 'shit' 'ass' 'bullshit'\n",
      " 'cunt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(stem_vectorizer.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_stemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/kw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/kw/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "# analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    return (WNlemma.lemmatize(t) for t in analyzer(doc))\n",
    "\n",
    "#lemm_vectorizer = CountVectorizer(analyzer=lemmatize_word)\n",
    "lemm_vectorizer = TfidfVectorizer(min_df=15, analyzer=lemmatize_word)\n",
    "\n",
    "# Transform X_train\n",
    "X_train_lemm_vectorized = lemm_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111699, 144313)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lemm_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.811\n"
     ]
    }
   ],
   "source": [
    "# Train the model with stemmed and vectorized dataset\n",
    "model_lemm = LogisticRegression(max_iter=1500)\n",
    "model_lemm.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model_lemm.predict(lemm_vectorizer.transform(X_test))\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['thank' 'talk' 'thanks' 'if' 'please' 'for' 'redirect' 'continue' 'help'\n",
      " 'welcome']\n",
      "\n",
      "Largest Coefs: \n",
      "['fuck' 'fucking' 'idiot' 'shit' 'stupid' 'as' 'suck' 'asshole' 'bullshit'\n",
      " 'crap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kw/neuefische/neuefische_exercises/toxic/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(lemm_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model_lemm.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print(\"Smallest Coefs:\\n{}\\n\".format(feature_names[sorted_coef_index[:10]]))\n",
    "print(\"Largest Coefs: \\n{}\".format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.811\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000005?line=7'>8</a>\u001b[0m predictions_SVM \u001b[39m=\u001b[39m SVM\u001b[39m.\u001b[39mpredict(lemm_vectorizer\u001b[39m.\u001b[39mtransform(X_test))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000005?line=8'>9</a>\u001b[0m \u001b[39m# Use accuracy_score function to get the accuracy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kw/neuefische/neuefische_exercises/toxic/data_mining.ipynb#ch0000005?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSVM Accuracy Score -> \u001b[39m\u001b[39m\"\u001b[39m,accuracy_score(predictions_SVM, test_Y)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_Y' is not defined"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', gamma='scale')\n",
    "SVM.fit(X_train_lemm_vectorized, y_train)\n",
    "\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(lemm_vectorizer.transform(X_test))\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "185399f65f1bf27befb6fa1360f967c397d6bbd796f4118422d85f5369bc8ec8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit ('3.9.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
