{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!pip install twokenize\n",
    "#import twokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223544</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>:Jerome, I see you never got around to this‚Ä¶! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223545</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223546</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223547</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223548</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "223544  fff8f64043129fa2  :Jerome, I see you never got around to this‚Ä¶! ...   \n",
       "223545  fff9d70fe0722906  ==Lucky bastard== \\n http://wikimediafoundatio...   \n",
       "223546  fffa8a11c4378854  ==shame on you all!!!== \\n\\n You want to speak...   \n",
       "223547  fffac2a094c8e0e2  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...   \n",
       "223548  fffb5451268fb5ba  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "223544      0             0        0       0       0              0  \n",
       "223545      0             0        0       0       0              0  \n",
       "223546      0             0        0       0       0              0  \n",
       "223547      1             0        1       0       1              0  \n",
       "223548      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wikipedia=pd.read_csv(\"../data/train_wikipedia.csv\")\n",
    "#df2 = {'id': 31234234, 'comment_text': 'I love Pizza üçï! Shall we book a cab üöï to gizza?', 'toxic':0,\n",
    "#'severe_toxic':0,'obscene':1,'threat':1,'insult':1,'identity_hate':1}\n",
    "#df_wikipedia = df_wikipedia.append(df2, ignore_index = True)\n",
    "df_wikipedia.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove double quotations \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\\nMore\\nI can't make any real suggestions on i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \\nMore\\nI can't make any real suggestions on i...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_doublequotation(panda_text_column):\n",
    "    return panda_text_column.apply(lambda x: x.replace('\"', ''))\n",
    "df_wikipedia['comment_text'] = remove_doublequotation(df_wikipedia['comment_text'])\n",
    "df_wikipedia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Non-Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nontext(panda_column):\n",
    "    return panda_column.apply(lambda x: x.encode(\"utf-8\"))\n",
    "#df_wikipedia[\"comment_text\"] = handle_nontext(df_wikipedia[\"comment_text\"] )\n",
    "#df_wikipedia.tail()\n",
    "\n",
    "# then bytes make it diffcult to deal with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling apostrophes (manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apostrophes_expansion = {\n",
    "\"i'm\": \"i am\",\"I'm\": \"I am\", \"I'M\": \"I AM\", \n",
    "\"i'd\": \"i would\", \"I'd\": \"I would\",\"I'D\": \"I WOULD\",\n",
    "\"you're\": \"you are\", \"You're\": \"You are\", \"YOU'RE\": \"YOU ARE\",\n",
    "\"he's\": \"he is\",\"He's\": \"He is\",\"HE'S\": \"HE IS\",\n",
    "\"she's\": \"she is\",\"She's\": \"She is\",\"SHE'S\": \"SHE IS\",\n",
    "\"it's\":\"it is\", \"It's\":\"It is\", \"IT'S\":\"IT IS\",\n",
    "\"that's\": \"that is\", \"That's\": \"That is\",\"THAT'S\": \"THAT IS\",\n",
    "\"they're\": \"they are\",\"They're\": \"They are\",\"THEY'RE\": \"THEY ARE\",\n",
    "\"we're\": \"we are\",\"We're\": \"We are\",\"WE'RE\": \"WE ARE\",\n",
    "\"i've\": \"i have\", \"I've\": \"I have\", \"I'VE\": \"I HAVE\",\n",
    "\"you've\": \"you have\",\"You've\": \"You have\",\"YOU'VE\": \"YOU HAVE\",\n",
    "\"we've\": \"we have\",\"We've\": \"We have\",\"WE'VE\": \"WE HAVE\",\n",
    "\"ain't\": \"are not\",\"Ain't\": \"Are not\",\"AIN'T\": \"ARE NOT\",\n",
    "\"aren't\": \"are not\", \"Aren't\": \"Are not\",\"AREN'T\": \"ARE NOT\",\n",
    "\"isn't\": \"is not\", \"Isn't\": \"Is not\", \"ISN'T\": \"IS NOT\",\n",
    "\"don't\": \"do not\", \"Don't\": \"Do not\",\"DON'T\": \"DO NOT\",\n",
    "\"doesn't\": \"does not\", \"Doesn't\": \"Does not\", \"DOESN'T\": \"DOES NOT\",\n",
    "\"won't\": \"will not\",\"Won't\": \"Will not\", \"WON'T\": \"WILL NOT\",\n",
    "\"can't\": \"cannot\",\"Can't\": \"Cannot\", \"CAN'T\": \"CANNOT\",\n",
    "\"shouldn't\": \"should not\",\"Shouldn't\": \"Should not\",\"SHOULDN'T\": \"SHOULD NOT\",\n",
    "\"wouldn't\": \"would not\", \"Wouldn't\": \"Would not\", \"WOULDN'T\": \"WOULD NOT\",\n",
    "\"didn't\": \"did not\", \"Didn't\": \"Did not\", \"DIDN'T\": \"DID NOT\",\n",
    "\"weren't\":\"were not\",\"Weren't\":\"Were not\", \"WEREN'T\":\"WERE NOT\", \n",
    "\"wasn't\":\"was not\",\"Wasn't\":\"Was not\", \"WASN'T\":\"WAS NOT\", \n",
    "\"there's\": \"there is\", \"There's\": \"There is\", \"THERE'S\": \"THERE IS\",\n",
    "\"here's\": \"here is\", \"Here's\": \"Here is\", \"HERE'S\": \"HERE IS\",\n",
    "\"let's\": 'let us', \"Let's\": \"Let us\", \"LET'S\": \"LET US\"\n",
    "} \n",
    "\n",
    "def interpolate_apostrophes_string(string):\n",
    "    x = string.split(' ')\n",
    "    return \" \".join([Apostrophes_expansion[word] if word in Apostrophes_expansion else word for word in x])\n",
    "def interpolate_apostrophes_column(panda_text_column):\n",
    "    return panda_text_column.apply(lambda x: interpolate_apostrophes_string(x))\n",
    "df_wikipedia['comment_text'] = interpolate_apostrophes_column(df_wikipedia.comment_text)\n",
    "df_wikipedia.to_csv('../data/test_apostrophes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonstandard spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "def correct_spelling(panda_text_column):\n",
    "    from textblob import TextBlob\n",
    "    return panda_text_column.apply(lambda x: TextBlob(x).correct())\n",
    "\n",
    "df_wikipedia['comment_text'] = correct_spelling(df_wikipedia['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I am ...\n",
       "2    Hey man, I am really not trying to edit war. I...\n",
       "3    \\nMore\\nI cannot make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wikipedia['comment_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages (2022.3.15)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'regex_or' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=23'>24</a>\u001b[0m     string \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(URL_regex2, \u001b[39m\"\u001b[39m\u001b[39m constant_url \u001b[39m\u001b[39m\"\u001b[39m, string) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m string\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=26'>27</a>\u001b[0m process_URLs(\u001b[39m\"\u001b[39;49m\u001b[39mstrinHey man, I am reallg www.facebook.com\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb Cell 13'\u001b[0m in \u001b[0;36mprocess_URLs\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_URLs\u001b[39m(string): \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=2'>3</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=3'>4</a>\u001b[0m \u001b[39m    replace all URLs in the tweet text\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=4'>5</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=5'>6</a>\u001b[0m     UrlStart1 \u001b[39m=\u001b[39m regex_or(\u001b[39m'\u001b[39m\u001b[39mhttps?://\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwww\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=6'>7</a>\u001b[0m     CommonTLDs \u001b[39m=\u001b[39m regex_or( \u001b[39m'\u001b[39m\u001b[39mcom\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mco\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m.uk\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39morg\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mnet\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mca\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mbiz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=7'>8</a>\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39medu\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39min\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mau\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000042?line=8'>9</a>\u001b[0m     UrlStart2 \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[a-z0-9\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.-]+?\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m CommonTLDs \u001b[39m+\u001b[39m pos_lookahead(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[/ \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regex_or' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install regex\n",
    "def process_URLs(string): \n",
    "    '''\n",
    "    replace all URLs in the tweet text\n",
    "    '''\n",
    "    UrlStart1 = regex_or('https?://', r'www\\.')\n",
    "    CommonTLDs = regex_or( 'com','co\\\\.uk','org','net','info','ca','biz',\n",
    "                                'info','edu','in','au')\n",
    "    UrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + pos_lookahead(r'[/ \\W\\b]')\n",
    "    \n",
    "    # * not + for case of: \"go to bla.com.\" -- don't want period UrlBody = r'[^ \\t\\r\\n<>]*?'\n",
    "    UrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity) \n",
    "    UrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n",
    "    Url = (optional(r'\\b') +\n",
    "                regex_or(UrlStart1, UrlStart2) +\n",
    "                UrlBody +\n",
    "    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n",
    "    \n",
    "    Url_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n",
    "    string = re.sub(Url_RE, \" constant_url \", string)\n",
    "    \n",
    "    # fix to handle unicodes in URL\n",
    "    URL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*' \n",
    "    string = re.sub(URL_regex2, \" constant_url \", string) \n",
    "    return string\n",
    "\n",
    "process_URLs(\"strinHey man, I am reallg www.facebook.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate emojis into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wp/ylcd0z3n66jgx_wrcv4p13v80000gn/T/ipykernel_75753/621620625.py:3: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wp/ylcd0z3n66jgx_wrcv4p13v80000gn/T/ipykernel_75753/621620625.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_wikipedia['comment_text'][i] = text.replace(item, '')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=12'>13</a>\u001b[0m \u001b[39m#print(demoji.findall(tweet))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=13'>14</a>\u001b[0m \u001b[39m#def get_emojis(panda_text_column):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=14'>15</a>\u001b[0m \u001b[39m#    return panda_text_column.apply(lambda x: demoji.findall(x))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=21'>22</a>\u001b[0m \u001b[39m#                panda_text_column[i] = text.replace(item, '')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=22'>23</a>\u001b[0m \u001b[39m#remove_emojis(df_wikipedia['comment_text'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(df_wikipedia[\u001b[39m'\u001b[39m\u001b[39mcomment_text\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=24'>25</a>\u001b[0m     dem \u001b[39m=\u001b[39m demoji\u001b[39m.\u001b[39;49mfindall(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m dem:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kw/neuefische/detoxifying-my-social-media/notebooks/preprocessing_kat.ipynb#ch0000040?line=26'>27</a>\u001b[0m         \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m dem\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py:84\u001b[0m, in \u001b[0;36mcache_setter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=80'>81</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=81'>82</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=82'>83</a>\u001b[0m     set_emoji_pattern()\n\u001b[0;32m---> <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=83'>84</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py:99\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=88'>89</a>\u001b[0m \u001b[39m@cache_setter\u001b[39m\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=89'>90</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfindall\u001b[39m(string):\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=90'>91</a>\u001b[0m     \u001b[39m\"\"\"Find emojis within ``string``.\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=91'>92</a>\u001b[0m \n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=92'>93</a>\u001b[0m \u001b[39m    :param string: The input text to search\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=95'>96</a>\u001b[0m \u001b[39m    :rtype: dict\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=96'>97</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/kw/neuefische/detoxifying-my-social-media/.venv/lib/python3.9/site-packages/demoji/__init__.py?line=98'>99</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {f: _CODE_TO_DESC[f] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(_EMOJI_PAT\u001b[39m.\u001b[39;49mfindall(string))}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!pip install demoji\n",
    "#import demoji\n",
    "demoji.download_codes()\n",
    "# get dictionary of emojis\n",
    "def get_emojis(panda_text_column):\n",
    "    dict = {}\n",
    "    for row in panda_text_column:\n",
    "        dict.update(demoji.findall(row))\n",
    "    return dict\n",
    "x = get_emojis(df_wikipedia['comment_text'][:100])\n",
    "print(x)\n",
    "#print(demoji.findall(tweet))\n",
    "#def get_emojis(panda_text_column):\n",
    "#    return panda_text_column.apply(lambda x: demoji.findall(x))\n",
    "#x = get_emojis(df_wikipedia['comment_text'][:1000] )\n",
    "\n",
    "\n",
    "def remove_emojis(df, column):\n",
    "    df_copy = df.copy()\n",
    "    for i,text in enumerate(df_copy[column]):\n",
    "        dem = demoji.findall(text)\n",
    "        if dem:\n",
    "            for item in dem.keys():\n",
    "                df_copy[column][i] = text.replace(item, '')\n",
    "    return df_copy\n",
    "\n",
    "remove_emojis(df_wikipedia, 'comment_text'])\n",
    "for i,text in enumerate(df_wikipedia['comment_text']):\n",
    "    dem = demoji.findall(text)\n",
    "    if dem:\n",
    "        for item in dem.keys():\n",
    "            df_wikipedia['comment_text'][i] = text.replace(item, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love Pizza ! Shall we book a cab  to gizza?'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I love Pizza üçï! Shall we book a cab üöï to gizza?\"\n",
    "dem = demoji.findall(tweet)\n",
    "for item in dem.keys():\n",
    "    tweet = tweet.replace(item, '')\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChainMap(0      {}\n",
       "1      {}\n",
       "2      {}\n",
       "3      {}\n",
       "4      {}\n",
       "       ..\n",
       "995    {}\n",
       "996    {}\n",
       "997    {}\n",
       "998    {}\n",
       "999    {}\n",
       "Name: comment_text, Length: 1000, dtype: object)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import ChainMap\n",
    "d6 = ChainMap(x)\n",
    "d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wikipedia.to_csv('train_wikipedia_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad347432433d227a6a1af674451af37b20c66c076ff8f20805b46e8b7d486412"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
