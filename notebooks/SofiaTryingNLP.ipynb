{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import TRACKING_URI, EXPERIMENT_NAME\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_data():\n",
    "    DATA_NAME = 'wikipedia_pre_clean'\n",
    "    train = pd.read_csv(\"./data/train_\" + DATA_NAME + \".csv\")\n",
    "    test = pd.read_csv(\"./data/test_\" + DATA_NAME + \".csv\")\n",
    "    # cleaning data and preparing\n",
    "    Y = train[\"toxic\"]\n",
    "    X = train[\"comment_text\"]\n",
    "    Y_test = test[\"toxic\"]\n",
    "    X_test= test[\"comment_text\"]\n",
    "    return X,Y, X_test, Y_test, DATA_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    metrics = [roc_auc_score, precision_score]\n",
    "    classifier = LogisticRegression()\n",
    "    cv = cross_validate(classifier, X_train, y_train, scoring = metrics)\n",
    "    scores = {metric: cv[f'test_{metric}'] for metric in metrics}\n",
    "    # log all the stuff here\n",
    "    mlflow.log_metric(\"ROC\", scores[0])\n",
    "    mlflow.log_metric(\"Precision\", scores[1])\n",
    "\n",
    "    mlflow.sklearn.log_model(classifier.fit(X, Y))\n",
    "    return scores['some_loss'].mean()\n",
    "\n",
    "space = hp.choice(...)\n",
    "trials = SparkTrials(parallelism = ...)\n",
    "with mlflow.start_run() as run:\n",
    "    best_result = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 100, trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3e6334f05070679b9502e85617ba58e7b58462f45dc19b7a70de4bad0850bef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
